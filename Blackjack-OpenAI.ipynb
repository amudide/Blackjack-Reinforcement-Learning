{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing dependencies\n",
    "import gym\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Blackjack environment\n",
    "env = gym.make('Blackjack-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuple(Discrete(32), Discrete(11), Discrete(2))\n",
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "# Displaying the space of all possible states and actions\n",
    "# 32 * 11 * 2 different states (32 different card sums for player, 11 different face up dealer cards, 2 possibilities for number of aces in player possesios)\n",
    "print(env.observation_space)\n",
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random agent randomly \"hits\" or \"sticks\" on each state\n",
    "\n",
    "class RandomAgent:\n",
    "    def __init__(self, env):\n",
    "        self.num_states = 32 * 11 * 2\n",
    "        self.num_actions = env.action_space.n\n",
    "    \n",
    "    def getAction(self, state):\n",
    "        return random.choice(range(self.num_actions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model-free Q-learning Agent learns from playing episodes\n",
    "\n",
    "class QAgent:\n",
    "    def __init__(self, env):\n",
    "        self.num_states = 32 * 11 * 2\n",
    "        self.num_actions = env.action_space.n\n",
    "        self.qtable = np.zeros([32 * 11 * 2, 2])\n",
    "    \n",
    "    # Hash table from state to integer\n",
    "    def index(self, state):\n",
    "        return state[0] + state[1] * 32 + state[2] * 32 * 11\n",
    "    \n",
    "    # Train function. Parameters such as num_episodes, learning_rate, discount_rate, min_exp (min exploration rate) can be tweeked\n",
    "    \n",
    "    def train(self, num_episodes = 500000, max_steps = 8, learning_rate = 0.15, discount_rate = 1, min_exp = 0.001):\n",
    "        # Initialize default exploration rate. This represents probability that the agent will \"explore\" instead of \"exploit\"\n",
    "        exploration_rate = 1\n",
    "        # Learn state-action values from ~500,000 games\n",
    "        for episode in range(num_episodes):\n",
    "            state = env.reset()\n",
    "            done = None\n",
    "\n",
    "            for step in range(max_steps): \n",
    "                # Choose an action randomly with probability (exploration_rate); choose action with highest Q-value otherwise.\n",
    "                sample = random.uniform(0, 1)\n",
    "                if sample > exploration_rate:\n",
    "                    action = np.argmax(self.qtable[self.index(state), :]) \n",
    "                else:\n",
    "                    action = env.action_space.sample()\n",
    "            \n",
    "                # Take action\n",
    "                next_s, reward, done, info = env.step(action)\n",
    "        \n",
    "                # Update Q-value according to Bellmann Equation\n",
    "                self.qtable[self.index(state), action] = (1 - learning_rate) * self.qtable[self.index(state), action] + learning_rate * (reward + discount_rate * np.max(self.qtable[self.index(next_s), :]))\n",
    "        \n",
    "                # Transition to next state\n",
    "                state = next_s\n",
    "        \n",
    "                if done == True: \n",
    "                    break\n",
    "            \n",
    "            # Update exploration rate (exponential decay with a minimum exploration rate)\n",
    "            exploration_rate = min_exp + (1 - min_exp) * np.exp(-episode * 0.001)    \n",
    "            \n",
    "    def getAction(self, state):\n",
    "        return np.argmax(self.qtable[self.index(state), :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAINING. With default parameters, make take up to a couple of minutes. Use commented line instead of 4th line for faster (but less optimal) training\n",
    "\n",
    "state = env.reset()\n",
    "q_agent = QAgent(env)\n",
    "# q_agent.train(num_episodes = 100000)\n",
    "q_agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win rate: 0.4005\n",
      "Loss rate: 0.514\n",
      "Tie rate: 0.0855\n",
      "Payout over 10,000 games: -1135.0\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "\n",
    "# Compare with a random agent\n",
    "r_agent = RandomAgent(env)\n",
    "\n",
    "# Test on 10,000 games\n",
    "testing_episodes = 10000\n",
    "max_steps = 6\n",
    "\n",
    "num_wins = 0\n",
    "num_losses = 0\n",
    "num_ties = 0\n",
    "\n",
    "payout = 0\n",
    "\n",
    "for episode in range(testing_episodes):\n",
    "    state = env.reset()\n",
    "    cur_reward = 0\n",
    "    \n",
    "    for step in range(max_steps):\n",
    "        # action = r_agent.getAction(state)\n",
    "        action = q_agent.getAction(state) # Get action from Q agent\n",
    "        \n",
    "        # Apply action\n",
    "        state, reward, done, info = env.step(action)\n",
    "        \n",
    "        cur_reward += reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "    # Update wins/losses/ties according to reward\n",
    "    if (cur_reward == 1):\n",
    "        num_wins += 1\n",
    "    elif (cur_reward == 0):\n",
    "        num_ties += 1\n",
    "    else:\n",
    "        num_losses += 1\n",
    "        \n",
    "    payout += cur_reward\n",
    "        \n",
    "# Print diagnostics\n",
    "print(\"Win rate:\", num_wins / testing_episodes)\n",
    "print(\"Loss rate:\", num_losses / testing_episodes)\n",
    "print(\"Tie rate:\", num_ties / testing_episodes)\n",
    "print(\"Payout over 10,000 games:\", payout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State is described as (sum of player cards, face up dealer card, whether or not player has an ace).\n",
      "Start State: (11, 6, False)\n",
      "Agent Hits\n",
      "State after Hitting\n",
      "(21, 6, False)\n",
      "Agent Sticks\n",
      "State after Sticking\n",
      "(21, 6, False)\n",
      "\n",
      " Final Reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to see the agent play!\n",
    "\n",
    "state = env.reset()\n",
    "reward = -10\n",
    "\n",
    "print(\"State is described as (sum of player cards, face up dealer card, whether or not player has an ace).\")\n",
    "print(\"Start State:\", state)\n",
    "\n",
    "names = [\"Sticks\", \"Hits\"]\n",
    "participles = [\"Sticking\", \"Hitting\"]\n",
    "\n",
    "for i in range(6):\n",
    "    action = q_agent.getAction(state)\n",
    "    \n",
    "    print(\"Agent\", names[action])\n",
    "    \n",
    "    state, reward, done, info = env.step(action)\n",
    "    \n",
    "    print(\"State after\", participles[action])\n",
    "    print(state)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "        \n",
    "print(\"\\n Final Reward:\", reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
